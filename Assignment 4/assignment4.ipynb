{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS677 Machine Learning Assignment #4\n",
    "\n",
    "Aayushi Verma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question One\n",
    "For given confusion matrix calculate precision, recall and f1 score(manual calculations based on formula shared in slides from week#9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 30\n",
    "fp = 30\n",
    "fn = 10\n",
    "tn = 930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tp, fp, fn, tn):\n",
    "    numerator = tp + tn\n",
    "    denominator = tp + tn + fp + fn\n",
    "    accuracy = numerator / denominator\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp, fn, tn):\n",
    "    numerator = tp\n",
    "    denominator = tp + fp\n",
    "    precision = numerator / denominator\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp, fp, fn, tn):\n",
    "    numerator = tp\n",
    "    denominator = tp + fn\n",
    "    recall = numerator / denominator\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(tp, fp, fn, tn):\n",
    "    p = precision(tp, fp, fn, tn)\n",
    "    r = recall(tp, fp, fn, tn)\n",
    "    numerator = p * r\n",
    "    denominator = p + r\n",
    "    f1 = 2 * (numerator / denominator)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.96\n",
      "Precision = 0.5\n",
      "Recall = 0.75\n",
      "F1-Score = 0.6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy = {accuracy(tp, fp, fn, tn)}\")\n",
    "print(f\"Precision = {precision(tp, fp, fn, tn)}\")\n",
    "print(f\"Recall = {recall(tp, fp, fn, tn)}\")\n",
    "print(f\"F1-Score = {f1(tp, fp, fn, tn)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Two\n",
    "**How does the Support Vector Machine (SVM) algorithm use the concept of margin and decision boundary to achieve optimal classification in machine learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification problems, data points are represented as an underlying vector space. A hypersurface divides this vector space into partitions, where each part represents the class of the data points contained within. This hypersurface is called the decision boundary. The space between the descision boundary and the nearest data points is called the margin. \n",
    "\n",
    "The Support Vector Machine (SVM) algorithm is very powerful in terms of being able to perform linear or nonlinear classification, regression, and outlier detection tasks. It uses the kernel trick, which means it transforms data into a higher-dimensional space to make the data linearly separable and hence easier to solve. It builds the classifier with a descision boundary that is furthest from any data point, called support vectors. The margin is the distance between the left and right hyperplanes, i.e. between the support vectors. The support vectors are used to maximize the margin of the classifier, which optimizes the classifier decision boundary.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Three\n",
    "**How does the decision tree algorithm work in machine learning, and how can it be used for both classification and regression tasks? What are the hyperparameters of a decision tree algorithm and how do they affect the performance of the model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART (Classification and Regression Tree) analysis refers to a decision tree algorithm which has either a discrete class prediction, or a real-valued number prediction, i.e. a decision tree algorithm which can perform either categorical classification or regression. It recursively builds a decision tree to recursively split the dataset into subsets based on features. It then predicts values by traversing the decision tree from its root node to its leaf node and testing the value at each branch.\n",
    "\n",
    "Some hyperparameters include: \n",
    "- max_depth, which is the maximum allowed depth of the tree. Shallow trees are prone to underfitting, while deep trees are prone to overfitting.\n",
    "- min_samples_split, which is the minimum number of samples needed to split a node. A larger value can help in preventing noise.\n",
    "- min_samples_leaf, which is the minimum number of samples needed to be in a leaf node. A smaller value can help in preventing noise.\n",
    "- max_features, which is the number of features considered at each split. A smaller value can help in preventing noise.\n",
    "- criterion, which is the quality of a split, evaluated using Gini impurity or entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Four\n",
    "**Explain the basic steps of the k-means clustering algorithm and discuss how the choice of k can impact the quality of the resulting clusters in machine learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering is an unsupervised clustering algorithm that groups data points together with similar qualities. It is based on centroids and Euclidean distance, and the value *k*, which is the number of clusters. The algorithm starts by initializing the centroids randomly, and then finding the mean distance of points from the centroid. This is an iterative process, and continues until all data points have been clustered into the *k* clusters that was initially specified. The value of *k* affects the number of clusters and how the data points have been grouped into them. The elbow method is a good technique to find an optimal *k*, which will group the data points into groups without too much overlap and where each group has meaningful and differentiable characteristics that describe the data points contained within."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
